{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Img_class_kaggle_main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "848ac1d0"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n"
      ],
      "id": "848ac1d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edhqTHuVyX5P",
        "outputId": "7e1b67b2-109c-4f7b-81f6-16d05d3663a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive' , force_remount=True)"
      ],
      "id": "edhqTHuVyX5P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baf984c2"
      },
      "source": [
        "class CNN_classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() # To avoid referring to the base class explicitly \n",
        "        # (1) conv.layer 1\n",
        "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3)\n",
        "        self.conv1_bn = nn.BatchNorm2d(32)\n",
        "        # (2) conv.layer 2\n",
        "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)\n",
        "        self.conv2_bn = nn.BatchNorm2d(64)\n",
        "        # (3) conv.layer 3\n",
        "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3)\n",
        "        self.conv3_bn = nn.BatchNorm2d(128)\n",
        "        # (4) conv.layer 4\n",
        "        self.conv4 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 5)\n",
        "        self.conv4_bn = nn.BatchNorm2d(256)\n",
        "        # (5) conv.layer 5\n",
        "        self.conv5 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 5)\n",
        "        self.conv5_bn = nn.BatchNorm2d(512)\n",
        "        \n",
        "        # (1) fully connected layer 1\n",
        "        self.fc1 = nn.Linear(in_features=4*4*512, out_features=4000)\n",
        "        self.fc1_bn = nn.BatchNorm1d(4000)\n",
        "        self.dropout1=nn.Dropout(0.25)\n",
        "        # (2) fully connected layer 2\n",
        "        self.fc2 = nn.Linear(in_features=4000, out_features=400)\n",
        "        self.fc2_bn = nn.BatchNorm1d(400)\n",
        "        self.dropout2=nn.Dropout(0.25)\n",
        "        \n",
        "        # output layer\n",
        "        self.out = nn.Linear(in_features=400, out_features=10)\n",
        "        \n",
        "    def forward(self, t):\n",
        "            \n",
        "        #conv. layers\n",
        "        t = self.conv1(t)\n",
        "        t = F.leaky_relu(t, 0.1)\n",
        "        t = self.conv1_bn(t)\n",
        "\n",
        "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "      \n",
        "        t = self.conv2(t)\n",
        "        t = F.leaky_relu(t, 0.1)\n",
        "        t = self.conv2_bn(t)\n",
        "\n",
        "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "        \n",
        "        t = self.conv3(t)\n",
        "        t = F.leaky_relu(t, 0.1)\n",
        "        t = self.conv3_bn(t)\n",
        "\n",
        "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "      \n",
        "        t = self.conv4(t)\n",
        "        t = F.leaky_relu(t, 0.05)\n",
        "        t = self.conv4_bn(t)\n",
        "\n",
        "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "\n",
        "        t = self.conv5(t)\n",
        "        t = F.leaky_relu(t, 0.05)\n",
        "        t = self.conv5_bn(t)\n",
        "\n",
        "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "\n",
        "        t = t.reshape(-1, 4*4*512)\n",
        "        t = self.fc1(t)\n",
        "        t = self.dropout1(t)\n",
        "        t = F.leaky_relu(t, 0.05)\n",
        "        t = self.fc1_bn(t)\n",
        "\n",
        "       \n",
        "        t = self.fc2(t)\n",
        "        t = self.dropout2(t)\n",
        "        t = F.leaky_relu(t, 0.05)\n",
        "        t = self.fc2_bn(t)\n",
        "\n",
        "        #output layer\n",
        "        t = self.out(t)\n",
        "        t = F.softmax(t, dim=1)\n",
        "        return t"
      ],
      "id": "baf984c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79c2421f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "eb0fc1df-d52d-49c8-a8e4-e27853e7b6e2"
      },
      "source": [
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "id": "79c2421f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9222366e"
      },
      "source": [
        "model = CNN_classifier().to(device)"
      ],
      "id": "9222366e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4f6573d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46477ec4-c453-4988-af93-a9c5d7c8b7f2"
      },
      "source": [
        "#train_dir=\"/content/drive/MyDrive/kaggledata/train_main/train_aug2/train/\"\n",
        "#test_dir=\"/content/drive/MyDrive/kaggledata/test/\"\n",
        "\n",
        "train_dir=\"/content/drive/MyDrive/Datasets/GNR/train_aug\"\n",
        "test_dir=\"/content/drive/MyDrive/Datasets/GNR/test\"\n",
        "torch.manual_seed(108)\n",
        "train = datasets.ImageFolder(train_dir, transform=transforms.Compose([transforms.ToTensor()]), target_transform=None, is_valid_file=None)\n",
        "print(len(train))\n",
        "train_set, val_set = torch.utils.data.random_split(train, [5000,500]) \n",
        "train_data = torch.utils.data.DataLoader(train_set, batch_size = 10, num_workers=2, shuffle = True)\n",
        "val_data = torch.utils.data.DataLoader(val_set, batch_size = 5, num_workers=2, shuffle = True)"
      ],
      "id": "c4f6573d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE459iGrd9jn"
      },
      "source": [
        "test_set = datasets.ImageFolder(root=test_dir, transform=transforms.Compose([transforms.ToTensor()]), target_transform=None, is_valid_file=None)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=10, shuffle=False)"
      ],
      "id": "YE459iGrd9jn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSjaf2xioWe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0921eb4-b5e5-4a5c-f982-ce9be9d3fce6"
      },
      "source": [
        "train_data"
      ],
      "id": "FSjaf2xioWe1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f13f6e60350>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3bfbfa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562a875a-7bdb-4e31-bd21-09db6f134970"
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum = 0.5)\n",
        "acc0=0\n",
        "acc1=0\n",
        "test_accuracy =0\n",
        "for epoch in range(30): #50 no. of epochs\n",
        "    total_loss=0\n",
        "    total_preds = 0\n",
        "    val_total_preds = 0\n",
        "    count = 0\n",
        "\n",
        "    for batch in train_data:\n",
        "        model.train()\n",
        "        images, labels = batch\n",
        "        optimizer.zero_grad()                                                  # assining the gradients value 0\n",
        "        preds = model(images.to(device))                                         # Pass Batch\n",
        "        loss = F.cross_entropy(preds, labels.to(device))                           # Calculate Loss\n",
        "        loss.backward()                                                        # Calculate Gradients\n",
        "        optimizer.step()                                                       # Update Weights\n",
        "        total_loss += loss.item()                                              # Calculating total loss \n",
        "        total_preds += preds.argmax(dim=1).eq(labels.to(device)).sum().item()      # Calculating total no. of correct preditions \n",
        "        count =count+1  \n",
        "    train_acc= (((float) ((total_preds)))/5000) *100\n",
        "    val_acc = val_total_preds/5\n",
        "    print(\"Epoch: {}, Training Loss: {:0.5f}, Training Accuracy: {:0.3f}% Validation Accuracy: {:0.3f}%\".format(epoch, total_loss, train_acc, test_accuracy))\n",
        "    "
      ],
      "id": "c3bfbfa3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Training Loss: 852.07594, Training Accuracy: 81.880% Validation Accuracy: 0.000%\n",
            "Epoch: 1, Training Loss: 775.72709, Training Accuracy: 93.780% Validation Accuracy: 0.000%\n",
            "Epoch: 2, Training Loss: 757.63276, Training Accuracy: 96.660% Validation Accuracy: 0.000%\n",
            "Epoch: 3, Training Loss: 746.63244, Training Accuracy: 98.340% Validation Accuracy: 0.000%\n",
            "Epoch: 4, Training Loss: 741.01235, Training Accuracy: 99.120% Validation Accuracy: 0.000%\n",
            "Epoch: 5, Training Loss: 740.68055, Training Accuracy: 98.920% Validation Accuracy: 0.000%\n",
            "Epoch: 6, Training Loss: 738.55102, Training Accuracy: 99.300% Validation Accuracy: 0.000%\n",
            "Epoch: 7, Training Loss: 736.46721, Training Accuracy: 99.460% Validation Accuracy: 0.000%\n",
            "Epoch: 8, Training Loss: 735.73957, Training Accuracy: 99.540% Validation Accuracy: 0.000%\n",
            "Epoch: 9, Training Loss: 735.82083, Training Accuracy: 99.480% Validation Accuracy: 0.000%\n",
            "Epoch: 10, Training Loss: 734.75720, Training Accuracy: 99.760% Validation Accuracy: 0.000%\n",
            "Epoch: 11, Training Loss: 734.09197, Training Accuracy: 99.820% Validation Accuracy: 0.000%\n",
            "Epoch: 12, Training Loss: 733.60107, Training Accuracy: 99.840% Validation Accuracy: 0.000%\n",
            "Epoch: 13, Training Loss: 732.99838, Training Accuracy: 99.860% Validation Accuracy: 0.000%\n",
            "Epoch: 14, Training Loss: 733.11968, Training Accuracy: 99.800% Validation Accuracy: 0.000%\n",
            "Epoch: 15, Training Loss: 732.63981, Training Accuracy: 99.900% Validation Accuracy: 0.000%\n",
            "Epoch: 16, Training Loss: 733.03713, Training Accuracy: 99.800% Validation Accuracy: 0.000%\n",
            "Epoch: 17, Training Loss: 732.83396, Training Accuracy: 99.840% Validation Accuracy: 0.000%\n",
            "Epoch: 18, Training Loss: 732.68728, Training Accuracy: 99.860% Validation Accuracy: 0.000%\n",
            "Epoch: 19, Training Loss: 732.43642, Training Accuracy: 99.920% Validation Accuracy: 0.000%\n",
            "Epoch: 20, Training Loss: 732.29640, Training Accuracy: 99.880% Validation Accuracy: 0.000%\n",
            "Epoch: 21, Training Loss: 731.73554, Training Accuracy: 99.940% Validation Accuracy: 0.000%\n",
            "Epoch: 22, Training Loss: 732.23121, Training Accuracy: 99.900% Validation Accuracy: 0.000%\n",
            "Epoch: 23, Training Loss: 731.86061, Training Accuracy: 99.960% Validation Accuracy: 0.000%\n",
            "Epoch: 24, Training Loss: 732.43591, Training Accuracy: 99.860% Validation Accuracy: 0.000%\n",
            "Epoch: 25, Training Loss: 731.72018, Training Accuracy: 99.940% Validation Accuracy: 0.000%\n",
            "Epoch: 26, Training Loss: 731.75631, Training Accuracy: 99.960% Validation Accuracy: 0.000%\n",
            "Epoch: 27, Training Loss: 731.78240, Training Accuracy: 99.920% Validation Accuracy: 0.000%\n",
            "Epoch: 28, Training Loss: 731.42040, Training Accuracy: 99.960% Validation Accuracy: 0.000%\n",
            "Epoch: 29, Training Loss: 731.35374, Training Accuracy: 99.980% Validation Accuracy: 0.000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFDh6WdpPykb"
      },
      "source": [
        "#torch.save(model, \"/content/drive/MyDrive/kaggledata/model_sm.pkl\")\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/Datasets/GNR/MS2\")"
      ],
      "id": "qFDh6WdpPykb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f52412ce"
      },
      "source": [
        "test_set = datasets.ImageFolder(root=test_dir, transform=transforms.Compose([transforms.ToTensor()]), target_transform=None, is_valid_file=None)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)"
      ],
      "id": "f52412ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02oBgD2XICKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fff128e-3e2b-47d6-8c07-dcbf2f372502"
      },
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Datasets/GNR/MS2\"))\n",
        "for i in test_loader:\n",
        "    my_image,label = i\n",
        "    prediction = model(my_image.to(device))\n",
        "    print(prediction.argmax(dim = 1))\n",
        "result = prediction.argmax(dim = 1)\n",
        "y_test=[4., 9., 2., 4., 1., 0., 9., 3., 1., 1., 0., 0., 4., 5., 0., 8., 7., 7.,\n",
        "        7., 5., 4., 1., 5., 3., 2., 5., 6., 6., 4., 2., 0., 0., 2., 7., 1., 3.,\n",
        "        2., 1., 6., 8., 9., 9., 0., 3., 6., 8., 9., 4., 6., 5., 7., 7., 0., 6.,\n",
        "        8., 3., 9., 2., 6., 1., 9., 2., 4., 1., 0., 3., 7., 8., 3., 9., 4., 6.,\n",
        "        8., 1., 7., 6., 6., 7., 2., 5., 1., 3., 1., 7., 6., 9., 7., 2., 4., 6.,\n",
        "        9., 1., 5., 8., 1., 6., 4., 3., 9., 7.]\n",
        "y_test = torch.tensor(np.array(y_test))\n",
        "print(torch.eq(y_test.to(device), result).sum())"
      ],
      "id": "02oBgD2XICKD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 9, 2, 4, 0, 0, 9, 3, 0, 1, 6, 0, 4, 5, 7, 8, 5, 4, 7, 7, 4, 3, 5, 3,\n",
            "        2, 5, 6, 6, 4, 2, 8, 0, 2, 7, 6, 3, 2, 1, 5, 8, 9, 9, 0, 3, 6, 3, 9, 4,\n",
            "        0, 8, 7, 7, 0, 5, 0, 3, 9, 2, 6, 6, 9, 5, 8, 3, 6, 3, 7, 8, 3, 9, 4, 5,\n",
            "        8, 6, 7, 2, 5, 0, 2, 8, 6, 3, 0, 7, 5, 9, 7, 2, 4, 2, 5, 1, 5, 8, 1, 6,\n",
            "        4, 4, 7, 7], device='cuda:0')\n",
            "tensor(66, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fc4a80f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1dd858-1b52-4093-879f-32f30feca072"
      },
      "source": [
        "y_pred = np.array(result.cpu()).astype(int)\n",
        "y_pred= y_pred+1\n",
        "print(y_pred)\n",
        "a = {'ImageID': range(101, 201), 'LabelID': y_pred}\n",
        "b = pd.DataFrame(a)\n",
        "print(b)\n",
        "b.to_csv('19D070070.csv', index = False)"
      ],
      "id": "8fc4a80f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 5 10  3  5  1  1 10  4  1  2  7  1  5  6  8  9  6  5  8  8  5  4  6  4\n",
            "  3  6  7  7  5  3  9  1  3  8  7  4  3  2  6  9 10 10  1  4  7  4 10  5\n",
            "  1  9  8  8  1  6  1  4 10  3  7  7 10  6  9  4  7  4  8  9  4 10  5  6\n",
            "  9  7  8  3  6  1  3  9  7  4  1  8  6 10  8  3  5  3  6  2  6  9  2  7\n",
            "  5  5  8  8]\n",
            "    ImageID  LabelID\n",
            "0       101        5\n",
            "1       102       10\n",
            "2       103        3\n",
            "3       104        5\n",
            "4       105        1\n",
            "..      ...      ...\n",
            "95      196        7\n",
            "96      197        5\n",
            "97      198        5\n",
            "98      199        8\n",
            "99      200        8\n",
            "\n",
            "[100 rows x 2 columns]\n"
          ]
        }
      ]
    }
  ]
}